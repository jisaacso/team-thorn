import re
import json
import argparse
from gensim import corpora, models, similarities, matutils
#from settings import stoplist
import numpy as np
#from cityhash import CityHash64
from simhash_db import Client
from redis import ConnectionError
from simhash_features import Bithash


def tfidf(text_corpus):
    """
    Translates a json text corpus into tf-idf vectors
    """
    corpus = []
    for doc in text_corpus:
        corpus.append(doc['object']['description'])

    print 'Lowering text'
    # lower text and remove stoplist
    texts = [[word for word in document.lower().split()] for document in corpus]
              #if word not in stoplist] for document in corpus]

    print 'Removing singular tokens'
    '''
    ## Todo: make not slow as a sloth ##

    # remove single tokens
    #all_tokens = sum(texts,[])
    #tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word)==1)
    #texts = [[word for word in text if word not in tokens_once] for text in texts]

    all_tokens = [word for doc in texts for word in doc]
    token_hist = Counter(all_tokens)
    tokens_once = [k for k in token_hist.keys() if token_hist[k] == 1]
    #tokens_once = Counter(all_tokens).keys
    texts = [[word for word in doc if word not in tokens_once] for doc in texts]
    '''

    print 'Building Tf-idf vectors'
    # build a tfidf dictionary
    dictionary = corpora.Dictionary(texts)
    bowcorpus = [dictionary.doc2bow(text) for text in texts]
    tfidf = models.TfidfModel(bowcorpus)
    tfidfcorpus = tfidf[bowcorpus]

    return tfidfcorpus

def tfidf_to_bithash(tfidf_corpus, nbits):
    """
    Convert tfidf features to a 'similarity hash' as described in
    "Detecting Near Duplicates...", Manku, et. al.

    For each doc, D in the corpus, a sim hash is generated by 64-bit hashing each
    Tf-idf feature, D_i. Every binary feature-hash is mapped to {-1, 1} and multiplied
    by the original Tf-idf weight. These feature-hashes are added. The simhash is
    finally binarized as sign(D_i).
    """
    print 'converting tf-idf to simhash'
    bithash_corpus = np.zeros(len(tfidf_corpus))

    for docidx, doc in enumerate(tfidf_corpus):
        running_hash = np.zeros(nbits)

        for f_rank, feature in doc:
            binhash = bin(CityHash64(str(feature)))

            if 'b' in binhash:
                binhash = binhash[2:]

            while(len(binhash) < nbits):
                binhash = '0' + binhash

            binhash_vec = np.array([int(v) for v in binhash], dtype=float)
            binhash_vec[binhash_vec == 0] = -1
            binhash_vec *= feature

            running_hash += binhash_vec

        zero_idxs = running_hash <= 0
        running_hash[zero_idxs] = 0
        running_hash[~zero_idxs] = 1

        bithash_corpus[docidx] = long(''.join([str(int(i)) for i in running_hash]), base=2)

    np.save('../data/bithash_corpus', bithash_corpus)

    return bithash_corpus

def find_duplicates(bithash_corpus, table_name, ntables, nmatchbits):

    try:
        redis_client = Client('redis', table_name, ntables, nmatchbits)
        redis_client.delete()
    except ConnectionError:
        print 'Connection error. Is redis running? Execute:\n>> redis-server'
        return -1

    # insert the first 20k documents
    redis_client.insert(bithash_corpus[:20000])

    # Find duplicates of the remaining ~2k documents
    dup_lists = redis_client.find_all(bithash_corpus[20001:])
    idxs_and_simhashs = [(20001+idx, dup_lists[idx]) for idx, val
                      in enumerate(dup_lists) if len(val)>0]

    return idxs_and_simhashs

def print_duplicates(duplicates, json_doc):

    for doc_idx, simhash_val in duplicates:
        duplicate_document = json_doc[doc_idx]['object']['description']
        original_document_idx = np.where(bithash_corpus == simhash_val)[0]
        if len(original_document_idx) > 0:
            print duplicate_document
            print '-'*40
            for od_idx in original_document_idx:
                original_document = json_doc[od_idx]['object']['description']
                print original_document
                print '-'*40
            print '='*40

if __name__ == '__main__':
    # python corpus_generator.py --json ../data/opentable.json --table opentable-test --bithash ../data/bithash_corpus.npy --ntables 6 --nmatchbits 3

    parser = argparse.ArgumentParser(description='Generate 64bit simhash featues '
                                                 'from a text corpus')
    parser.add_argument('--json', help='Path to json corpus', required=True)
    parser.add_argument('--table', help='Name of the Redis table into which the'
                                        'Corpus will be loaded', required=True)
    parser.add_argument('--bithash', help='(optional) Path to numpy bithash'
                                               ' of the corpus', required=False)
    parser.add_argument('--tfidf', help='Add flag if whether to parse documents using tfidf'
                                           'weighting', required=False)
    parser.add_argument('--ntables', help='(optional) Number of simhash tables'
                                           ' to permute', required=False, default=6)
    parser.add_argument('--nmatchbits', help='Max number of bits allowed to be different'
                                           ' for a positive dup', required=False, default=3)


    args = vars(parser.parse_args())
    json_doc = []
    with open(args['json'], 'r') as fin:
        for line in fin:
            d = dict()
            d['object'] = dict()
            d['object']['description'] = line
            json_doc.append(d)
        

    if args['bithash']:
        bithash_corpus = np.load(args['bithash'])
    elif args['tfidf']:
        raise Exception('ToDo: Fix this code to use simhash_features post-tfidf')
        tfidf_corpus = tfidf(json_doc)
        bithash_corpus = tfidf_to_bithash(tfidf_corpus, nbits=64)
    else:
        print 'Generating bithash corpus'
        bithash_corpus = np.zeros(len(json_doc))
        for idx, doc in enumerate(json_doc):
            bithash_corpus[idx] = Bithash(doc['object']['description']).value
        np.save('../data/bithash_corpus', bithash_corpus)
        print 'Finished bithash corpus'


    duplicates = find_duplicates(bithash_corpus, args['table'],
                                 int(args['ntables']), int(args['nmatchbits']))
    print '='*40
    print duplicates
    print_duplicates(duplicates, json_doc)


    # ToDo: plot distribution of xor distances between bit hashes!!!
    # ToDo: html docs from airbnb - boilerplate = False.
    # ToDo: add more than one field from json
    # ToDo: Gensim hash algo: corpora hash dictionary
    # ToDo: Kill exact duplicates (URL)?



